{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cd9d432-4e90-4f1e-9dfc-37f66a52e1a8",
   "metadata": {},
   "source": [
    "# Week 4: N-gram language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb198a98-901e-40b8-9887-65cffd5cf791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eedb2d-2fca-467c-bf43-1f22fd1919ee",
   "metadata": {},
   "source": [
    "The Brown Corpus comes preprocessed via word tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772be0d2-f300-49dc-bd50-f19d33aeb312",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = brown.words()\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f36d7b9-0223-42ed-943e-42210282dc00",
   "metadata": {},
   "source": [
    "For the purpose of experimentation, let's create a train/test split of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b821aff8-e2d2-4862-925f-9fab3dfca229",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset[:1000000]\n",
    "test_data = dataset[1000000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f798d5-78d0-4a6a-9b4a-1b7797260b90",
   "metadata": {},
   "source": [
    "### Train uni-gram language model\n",
    "\n",
    "Let's now start by reimplementing our bag-of-words model from last week, or our unigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1ea9c3-e72c-42b7-9f4d-f69039eb3bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_vocabulary(dataset):\n",
    "    types = list(set(dataset))\n",
    "    return types\n",
    "\n",
    "def unigram_lm(sequence_tokens, vocabulary):\n",
    "    BoW = {t: 1 for t in vocabulary}\n",
    "    vocab_size = len(vocabulary)\n",
    "    counts = dict(Counter(sequence_tokens))\n",
    "    total = sum(counts.values())\n",
    "    for token in BoW:\n",
    "        if token in counts:\n",
    "            BoW[token] = (BoW[token] + counts[token])/(total + vocab_size)\n",
    "        else:\n",
    "            BoW[token] = (BoW[token])/(total + vocab_size)\n",
    "    return BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48874e5b-420f-4ed5-8509-e9528fe61104",
   "metadata": {},
   "source": [
    "Let's fit our unigram model to our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b177bb-d70b-4d74-8fbf-91834503ecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_unigrams = unigram_lm(train_data, get_unigram_vocabulary(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce1d27-e79f-43e5-acaa-b7cb589ce184",
   "metadata": {},
   "source": [
    "### Train an bi-gram language model\n",
    "\n",
    "Now let's write a function that returns a bigram model. The first step is a function that returns the set of possible bigrams in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588ce01-2810-456b-b2bb-9ea667bfbf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_vocabulary(dataset):\n",
    "    bigram_types = []\n",
    "    pad_token = \"[PAD]\"\n",
    "    ## TO DO\n",
    "\n",
    "    ##\n",
    "    return bigram_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e19c36-816e-4979-8e93-6afcd325eca3",
   "metadata": {},
   "source": [
    "Now that we have a way to get the set of bigram types lets write the bigram model (don't forget to implement Laplace smoothing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76e98fe-5045-4a60-8578-47a7029e6418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_lm(train_data, dataset):\n",
    "    bigrams = get_bigram_vocabulary(dataset)\n",
    "    unigrams = get_unigram_vocabulary(dataset)+[\"[PAD]\"]\n",
    "    bigram_counts = {t: 1 for t in bigrams}\n",
    "    unigram_counts = {t: 0 for t in unigrams}\n",
    "    unigram_counts[\"[PAD]\"] = 1\n",
    "    bigram_probs = dict()\n",
    "    vocab_size = len(unigrams)\n",
    "    ## TO DO\n",
    "\n",
    "    ##\n",
    "    return bigram_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc91426-9282-4492-a847-662c21be580a",
   "metadata": {},
   "source": [
    "Let's fit a bigram model to the brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046bcbfe-5431-4160-8b09-08baec9345a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_bigrams = bigram_lm(train_data, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b1983-b058-4e33-b651-c45a77b5ff7f",
   "metadata": {},
   "source": [
    "### Train a tri-gram language model\n",
    "\n",
    "Lets now repeat these steps but for a trigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3324b8-0012-4538-8e47-9d07c6251ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigram_vocabulary(dataset):\n",
    "    trigram_types = []\n",
    "    pad_token = \"[PAD]\"\n",
    "    ## TO DO\n",
    "\n",
    "    ##\n",
    "    return trigram_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f83483-8994-47ba-b881-744a17f74727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_lm(train_data, dataset):\n",
    "    trigrams = get_trigram_vocabulary(dataset)\n",
    "    bigrams = get_bigram_vocabulary(dataset)+[(\"[PAD]\",\"[PAD]\")]\n",
    "    unigrams = get_unigram_vocabulary(dataset)+[\"[PAD]\"]\n",
    "    trigram_counts = {t: 1 for t in trigrams}\n",
    "    bigram_counts = {t: 0 for t in bigrams}\n",
    "    trigram_probs = dict()\n",
    "    vocab_size = len(unigrams)\n",
    "    ## TO DO\n",
    "\n",
    "    ##\n",
    "    return trigram_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e702c339-5b9e-4d0d-85de-61890c5bb5ab",
   "metadata": {},
   "source": [
    "Let's fit a trigram model to the brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f72911-c1e9-4397-8325-36e2e8dfe92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_trigrams = trigram_lm(train_data, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33937e13-1379-46a6-bffe-8e07ad12f939",
   "metadata": {},
   "source": [
    "### Compare the perplexity of the test data of each model\n",
    "\n",
    "Which of these models performs best at representing the test data distribution? Write a function that takes a fitted ngram model and a test dataset and returns the perplexity of that dataset. \n",
    "\n",
    "Since the probability of the test data is the product of the probabilities of the ngrams which compose it, it is a very small number and we risk running into a floating-point error when trying to compute it. Thus, we should calculate perplexity in log base 2 space. Here is the formula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2c6bd-393f-45e4-89db-192961152c9a",
   "metadata": {},
   "source": [
    "$PP(W) = 2^{-\\frac{1}{n}\\log P(W)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67663da8-0b9e-43ad-b7ee-f6c8767fd8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplexity(ngram_lm, test_data):\n",
    "    if type(list(ngram_lm.keys())[0]) is tuple :\n",
    "        ngram_size = len(list(ngram_lm.keys())[0])\n",
    "    else:\n",
    "        ngram_size = 1\n",
    "    perplexity = 0.0\n",
    "    n = len(test_data)+(ngram_size-1)\n",
    "    ## TO DO\n",
    "    \n",
    "    ##\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34a8dac-6458-44e3-88e8-c0126e304d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_perplexity(brown_trigrams, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eea59f0-ff16-4d86-b287-91805974b6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_perplexity_scores(models, dataset):\n",
    "    results = [get_perplexity(lm, dataset) for lm in models]\n",
    "    return results\n",
    "\n",
    "models = [brown_unigrams, brown_bigrams, brown_trigrams]\n",
    "train_perplexity = compare_perplexity_scores(models, train_data)\n",
    "test_perplexity = compare_perplexity_scores(models, test_data)\n",
    "\n",
    "results = {'models':['unigram','bigram','trigram'],\n",
    " 'train_perplexity':train_perplexity,\n",
    " 'test_perplexity':test_perplexity}\n",
    "\n",
    "df_results = pd.DataFrame(data=results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aa3b8b-df60-4a0d-8017-7e54a6bcd64b",
   "metadata": {},
   "source": [
    "**What do you notice about these results? Why might that be?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c5170b-2860-42d9-950b-f197ee517685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10: LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_scaling(logits, temperature=1.0):\n",
    "    probs = torch.softmax(logits / temperature, dim=-1)\n",
    "    return probs\n",
    "\n",
    "def greedy_decoding(probs):\n",
    "    return torch.argmax(probs, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark on MMLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will benchmark a LLM on the MMLU dataset. We will use the `tinyBenchmarks` dataset which contains a small subset of the MMLU dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we load the dataset\n",
    "ds = load_dataset(\"tinyBenchmarks/tinyMMLU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the first example\n",
    "dataset = ds[\"test\"]\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure each example has 4 choices\n",
    "for sample in dataset:\n",
    "    assert len(sample[\"choices\"]) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to encode each sample into a multiple choice format. Additionally, we need to create a chat template to format the input for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified encode function to create proper chat messages\n",
    "OPTIONS = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "def encode(examples):\n",
    "    inputs = {\"messages\": [], \"label\": []}\n",
    "    for idx, question in enumerate(examples[\"question\"]):\n",
    "        # Format the question and options\n",
    "        # The text should be the question followed by the options and an answer placeholder\n",
    "        # It should look like this:\n",
    "\n",
    "        # Question: What is the capital of France?\n",
    "        # A: Paris\n",
    "        # B: London\n",
    "        # C: Rome\n",
    "        # D: Madrid\n",
    "        # Answer:\n",
    "\n",
    "        # TODO: implement this\n",
    "        text = \"\"\n",
    "        \n",
    "        # Create chat messages\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ]\n",
    "        inputs[\"messages\"].append(messages)\n",
    "        inputs[\"label\"].append(OPTIONS[examples[\"answer\"][idx]])\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the map function to apply the encode function to the dataset\n",
    "dataset = dataset.map(encode, batched=True, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the first example again\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model and tokenizer\n",
    "model_name_or_path = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to apply the chat template to the dataset. We use the tokenizer for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the chat template to the dataset\n",
    "def apply_chat_template(examples):\n",
    "    return {\n",
    "        \"text\": [tokenizer.apply_chat_template(messages, tokenize=False) for messages in examples[\"messages\"]],\n",
    "        \"input_ids\": [tokenizer.apply_chat_template(messages, tokenize=True) for messages in examples[\"messages\"]]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(apply_chat_template, batched=True, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the first example again\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now generate predictions for each sample. We will iterate over the dataset one example at a time and generate predictions for each example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run forward pass\n",
    "N = 5 # we stop after N examples\n",
    "max_tokens = 10 # we generate max_tokens predictions for each example\n",
    "predictions = {} # collect predictions for each sample\n",
    "temperature = 0.7   \n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(dataset):\n",
    "        print(\"Generating predictions for sample\", idx)\n",
    "        predictions[idx] = []\n",
    "        input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0)\n",
    "\n",
    "        for _ in range(max_tokens):\n",
    "            # TODO: implement this\n",
    "            # Run a forward pass and get the logits for the last token\n",
    "            \n",
    "\n",
    "            # sample a token from the distribution\n",
    "            probs = temperature_scaling(logits_last_token, temperature=temperature)\n",
    "            preds = greedy_decoding(probs)\n",
    "\n",
    "            # convert prediction to token\n",
    "            preds_tokens = tokenizer.convert_ids_to_tokens(preds.tolist())\n",
    "            predictions[idx].append(preds_tokens)\n",
    "\n",
    "            # append prediction to input_ids\n",
    "            input_ids = torch.cat([input_ids, preds.unsqueeze(0)], dim=1)\n",
    "\n",
    "        if idx >= N:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy\n",
    "accuracies = []\n",
    "for idx in predictions:\n",
    "    # TODO: implement this\n",
    "    # Get the predictions for the sample and check whether the correct answer is in the predictions\n",
    "    correct = 0\n",
    "    accuracies.append(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy\n",
    "accuracy = sum(accuracies) / len(accuracies)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

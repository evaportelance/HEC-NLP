{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9: Contextualized embeddings and Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable the autoreload extension in Jupyter\n",
    "%load_ext autoreload\n",
    "# Set autoreload mode to 2, which automatically reloads modules before executing code\n",
    "# This means you don't need to restart the kernel when you modify imported Python files\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to install the following dependencies:\n",
    "```\n",
    "pip install datasets transformers evaluate\n",
    "pip install altair vegafusion vegafusion-python-embed vl-convert-python\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "import evaluate\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoModelForMaskedLM, BertForMaskedLM\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    BertForSequenceClassification,\n",
    ")\n",
    "from transformers import AutoTokenizer, BertTokenizer\n",
    "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "alt.data_transformers.enable(\"vegafusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextualized embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of this exercise will show how to extract contextualized embeddings from BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the Winogrande dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winogrande = load_dataset(\"coref-data/winogrande_raw\", \"winogrande_debiased\")\n",
    "winogrande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at an example from the dataset\n",
    "winogrande[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use huggingface transformers to load the BERT model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the model architecture\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to encode the training set. For this, we first need to write a function that encodes a list of sequences from the dataset. We then use the map function of the dataset to encode the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(examples):\n",
    "    # replace the _ with a [MASK] token\n",
    "    sentences = [sentence.replace(\"_\", tokenizer.mask_token) for sentence in examples[\"sentence\"]]\n",
    "    # encode the sentences (add special tokens, apply padding, apply truncation, set max length to 128)\n",
    "    sentence_ids = tokenizer.batch_encode_plus(sentences, add_special_tokens=True, padding=\"max_length\", truncation=True, max_length=128)\n",
    "    return sentence_ids\n",
    "\n",
    "# encode the training set\n",
    "data = winogrande[\"train\"].map(encode_sentence, batched=True, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the format to torch and only keep the input_ids, token_type_ids, and attention_mask\n",
    "data.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a dataloader to iterate over the dataset in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speed up the computations, we will subsample the dataset to 100 examples.\n",
    "indices = np.random.randint(0, len(data), size=100)\n",
    "data = data.select(indices)\n",
    "# Create a dataloader\n",
    "dataloader = DataLoader(data, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to extract the contextualized embeddings from BERT. For this, we first need to set the model to evaluation mode.\n",
    "Then, we can iterate over the dataloader and extract the contextualized embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # set model to evaluation mode\n",
    "\n",
    "# to collect embeddings and hidden representations, we initialize empty lists\n",
    "all_embeddings = []\n",
    "all_hidden_representations = []\n",
    "\n",
    "with torch.no_grad(): # disable gradient calculation. Not needed for inference\n",
    "    for batch in dataloader:\n",
    "        # compute embeddings\n",
    "        embeddings = model.bert.embeddings.word_embeddings(batch[\"input_ids\"])\n",
    "        all_embeddings.append(embeddings)\n",
    "        # run forward pass\n",
    "        bert_output = model(**batch, output_hidden_states=True, output_attentions=True) \n",
    "        # print(bert_output.logits.shape)\n",
    "        # print(len(bert_output.hidden_states), bert_output.hidden_states[0].shape)\n",
    "        # print(len(bert_output.attentions), bert_output.attentions[0].shape)\n",
    "        all_hidden_representations.append(bert_output.hidden_states[-1])\n",
    "\n",
    "# concatenate all embeddings\n",
    "all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "print(all_embeddings.shape)\n",
    "\n",
    "# concatenate all hidden representations\n",
    "all_hidden_representations = torch.cat(all_hidden_representations, dim=0)\n",
    "print(all_hidden_representations.shape)\n",
    "\n",
    "# create a flattened version of the embeddings by combining first two dimensions\n",
    "all_embeddings = all_embeddings.reshape(-1, all_embeddings.shape[-1])\n",
    "print(all_embeddings.shape)\n",
    "\n",
    "# create a flattened version of the hidden representations by combining first two dimensions\n",
    "all_hidden_representations = all_hidden_representations.reshape(-1, all_hidden_representations.shape[-1])\n",
    "print(all_hidden_representations.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we will visualize the representations of BERT in 2D using t-SNE. To make our life easier, we will limit ourselves to the tokens that are not [PAD] tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we collect the tokens for each sentence in the dataset\n",
    "all_tokens = []\n",
    "sentence_for_token = []\n",
    "for sample in data:\n",
    "    input_ids = sample[\"input_ids\"]\n",
    "    sentence_toks = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    # convert back to string\n",
    "    sentence = [tokenizer.decode([tidx for tidx in input_ids if tidx != tokenizer.pad_token_id])]\n",
    "    sentences = sentence * len(sentence_toks)\n",
    "    all_tokens.extend(sentence_toks)\n",
    "    sentence_for_token.extend(sentences)\n",
    "\n",
    "assert len(all_tokens) == len(sentence_for_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we filter out embeddings/representations of [PAD] tokens\n",
    "non_pad_indices = [i for i, token in enumerate(all_tokens) if token != \"[PAD]\"]\n",
    "non_pad_tokens = [all_tokens[i] for i in non_pad_indices]\n",
    "sentence_for_non_pad_token = [sentence_for_token[i] for i in non_pad_indices]\n",
    "\n",
    "assert len(all_tokens) == len(sentence_for_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to compute the t-SNE embeddings. T-SNE is a dimensionality reduction technique that is often used to visualize high-dimensional data in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TSNE\n",
    "# X = all_embeddings\n",
    "X = all_hidden_representations\n",
    "\n",
    "# Filter out embeddings of [PAD] tokens\n",
    "X = np.array([X[i] for i in non_pad_indices])\n",
    "print(X.shape)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\n",
    "X_2d = tsne.fit_transform(X)\n",
    "print(X_2d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to visualize the t-SNE embeddings using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize TSNE result\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "ax.scatter(X_2d[:, 0], X_2d[:, 1], marker=\".\", s=10)\n",
    "ax.set_xlabel(\"t-SNE dimension 1\")\n",
    "ax.set_ylabel(\"t-SNE dimension 2\")\n",
    "ax.set_title(\"2D projection of BERT representations\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create an interactive plot, we can use altair. Altair is a declarative statistical visualization library for Python. It's great for creating interactive plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For altair, we need to convert the data we want to plot to a pandas dataframe\n",
    "embeddings_df = pd.DataFrame(X_2d, columns=[\"x\", \"y\"])\n",
    "# add the tokens to the dataframe\n",
    "embeddings_df[\"token\"] = non_pad_tokens\n",
    "# add the sentence to the dataframe\n",
    "embeddings_df[\"sentence\"] = sentence_for_non_pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with altair\n",
    "alt.Chart(embeddings_df).mark_circle(size=40, opacity=0.5).encode(\n",
    "    x='x',\n",
    "    y='y',\n",
    "    # color='sentence',\n",
    "    tooltip=['sentence', 'token']\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=500\n",
    ").configure_legend(disable=True).interactive()\n",
    "\n",
    "# --> Hover over the points to see the token and sentence that generated the embedding!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning BERT for sequence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the model from the previous exercise\n",
    "if model is not None:\n",
    "    del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second part of this exercise will show how to fine-tune BERT for sequence classification. We will use the IMDB dataset, which is a commonly used benchmark for sentiment analysis of movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we load the dataset\n",
    "imdb = load_dataset(\"stanfordnlp/imdb\")\n",
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at an example from the dataset\n",
    "imdb[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the distribution of the labels in the training set\n",
    "labels = imdb[\"train\"][\"label\"]\n",
    "label_counts = Counter(labels)\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a random subset from the training set\n",
    "data = imdb[\"train\"].select(np.random.randint(0, len(imdb[\"train\"]), size=1000))\n",
    "label_counts = Counter(data[\"label\"])\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the BERT model for sequence classification and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we need to encode the sentences before we can fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(examples):\n",
    "    text = [t for t in examples[\"text\"]]\n",
    "    examples = tokenizer.batch_encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "    return examples\n",
    "\n",
    "\n",
    "data = data.map(encode_sentence, batched=True, batch_size=1000)\n",
    "data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"]\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fine-tune the model for a few steps using the HF trainer class. The HF trainer class takes care of the training loop, including learning rate scheduling, gradient accumulation, and evaluation. You can find more information about the trainer class in the [HF documentation](https://huggingface.co/docs/transformers/main/en/main_classes/trainer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a callback to collect the loss and history of the training logs\n",
    "class LossCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.logs = {}\n",
    "        self.global_steps = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        for k, v in logs.items():\n",
    "            if k not in self.logs:\n",
    "                self.logs[k] = []\n",
    "            self.logs[k].append(v)\n",
    "        if \"loss\" in logs:\n",
    "            self.losses.append(logs[\"loss\"])\n",
    "            self.global_steps.append(state.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the evaluate library to compute the accuracy and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We simply use the accuracy and F1 score from the evaluate library\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "\n",
    "# We define a function to compute the accuracy and F1 score given the predictions and labels\n",
    "def compute_metrics(eval_pred):\n",
    "    # Compute the predictions and labels\n",
    "    # eval_pred is a tuple of (predictions, labels)\n",
    "    predictions, labels = eval_pred\n",
    "    # Get the predicted class\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    # Compute the accuracy and F1 score\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=predictions, references=labels)[\n",
    "            \"accuracy\"\n",
    "        ],\n",
    "        \"f1\": f1.compute(\n",
    "            predictions=predictions, references=labels, average=\"weighted\"\n",
    "        )[\"f1\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LossCallback and the Trainer\n",
    "loss_callback = LossCallback()\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data,\n",
    "    eval_dataset=data,\n",
    "    callbacks=[loss_callback],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use matplotlib to visualize the loss during training\n",
    "global_steps = loss_callback.global_steps\n",
    "losses = loss_callback.losses\n",
    "print(global_steps)\n",
    "print(losses)\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.plot(losses)\n",
    "ax.set_xlabel(\"Logging step\")\n",
    "ax.set_xticks(range(len(global_steps)))\n",
    "ax.set_xticklabels(global_steps)\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Training Loss\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_callback.logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.state.log_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

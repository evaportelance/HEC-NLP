{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8: The Transformer and Huggingface transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable the autoreload extension in Jupyter\n",
    "%load_ext autoreload\n",
    "# Set autoreload mode to 2, which automatically reloads modules before executing code\n",
    "# This means you don't need to restart the kernel when you modify imported Python files\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Transformer architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this exercise is to implement the Transformer architecture in Pytorch. You should implement most of the operations from scratch. You can use the following classes:\n",
    "\n",
    "- `nn.Parameter` for the parameters\n",
    "- `nn.Linear` for the linear layers\n",
    "- `nn.Dropout` for the dropout\n",
    "- `nn.GELU` for the GELU activation function\n",
    "- `F.softmax` for the softmax function\n",
    "\n",
    "The following functions will be useful:\n",
    "- `torch.sqrt`\n",
    "- `torch.matmul`\n",
    "- `torch.triu`\n",
    "- `torch.var`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by implementing the layer normalization function. This is a function that normalizes the input by subtracting the mean and dividing by the standard deviation. \n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(\\mathbf{x}) = \\frac{\\mathbf{x} - \\text{E}[\\mathbf{x}]}{\\sqrt{\\text{Var}(\\mathbf{x}) + \\epsilon}} \\odot \\mathbf{\\gamma} + \\mathbf{\\beta}\n",
    "$$\n",
    "\n",
    "Note: Be careful with the unbiased flag in `torch.var()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.D = hidden_size\n",
    "        self.weight_D = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.bias_D = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape B x L x D\n",
    "\n",
    "        # TODO: Implement layer norm from scratch\n",
    "        \n",
    "        ### END TODO\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation by running the following cell\n",
    "# If successful, you should see: ✅ Test passed! Custom LayerNorm matches PyTorch's implementation.\n",
    "from week8_tests import test_layer_norm\n",
    "test_layer_norm(LayerNorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-forward layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement the feed-forward layer. \n",
    "\n",
    "$$\n",
    "\\text{MLP}(\\mathbf{x}) = \\mathbf{W}_2 \\text{GELU}(\\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1) + \\mathbf{b}_2\n",
    "$$\n",
    "\n",
    "Make sure to apply dropout after the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.W_in_DF = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.W_out_FD = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.activation_function = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape B x L x D\n",
    "\n",
    "        # TODO: Implement layer norm from scratch\n",
    "        \n",
    "        ### END TODO\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation by running the following cell\n",
    "# If successful, you should see: ✅ Test passed! MLP implementation is correct.\n",
    "from week8_tests import test_mlp\n",
    "test_mlp(MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will implement the self-attention layer. Self-attention is a mechanism that allows a Transformer to attend to different parts of the input sequence. It's the core of the Transformer architecture.\n",
    "\n",
    "$$\n",
    "\\text{SelfAttention}(\\mathbf{X}) = \\text{softmax}\\left(\\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_k}}\\right) \\mathbf{V}\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{Q}$, $\\mathbf{K}$, and $\\mathbf{V}$ are the queries, keys, and values, respectively.\n",
    "\n",
    "- $\\mathbf{Q}$ is obtained by multiplying $\\mathbf{X}$ with a weight matrix $\\mathbf{W}_q$\n",
    "- $\\mathbf{K}$ is obtained by multiplying $\\mathbf{X}$ with a weight matrix $\\mathbf{W}_k$\n",
    "- $\\mathbf{V}$ is obtained by multiplying $\\mathbf{X}$ with a weight matrix $\\mathbf{W}_v$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout=0.1, causal=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads  # H\n",
    "        self.head_dim = hidden_size // num_heads  # K\n",
    "        self.scale = self.head_dim**-0.5\n",
    "        self.causal = causal\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Define weights\n",
    "        self.W_q_DD = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_k_DD = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_v_DD = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_out_DD = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, D = x.shape  # x is of shape B x L x D\n",
    "        H, K = self.num_heads, self.head_dim\n",
    "\n",
    "        # 1. Create the queries, keys and values\n",
    "        # TODO: implement\n",
    "        q_BLD = None\n",
    "        k_BLD = None\n",
    "        v_BLD = None\n",
    "        # END TODO\n",
    "\n",
    "        # Reshape to separate the heads\n",
    "        # From (B x L x D) to (B x L x H x K)\n",
    "        q_BLHK = q_BLD.view(B, L, H, K)  # (B x L x D) -> (B x L x H x K)\n",
    "        k_BLHK = k_BLD.view(B, L, H, K)  # (B x L x D) -> (B x L x H x K)\n",
    "        v_BLHK = v_BLD.view(B, L, H, K)  # (B x L x D) -> (B x L x H x K)\n",
    "\n",
    "        # Transpose to get (B x H x L x K)\n",
    "        q_BHLK = q_BLHK.transpose(1, 2)  # (B x L x H x K) -> (B x H x L x K)\n",
    "        k_BHLK = k_BLHK.transpose(1, 2)  # (B x L x H x K) -> (B x H x L x K)\n",
    "        v_BHLK = v_BLHK.transpose(1, 2)  # (B x L x H x K) -> (B x H x L x K)\n",
    "\n",
    "        # 2. Compute attention weights\n",
    "        # Matmul q and k: (B x H x L x K) x (B x H x K x L) -> (B x H x L x L)\n",
    "        # TODO: implement\n",
    "        attn_BHLL = None\n",
    "        # END TODO\n",
    "\n",
    "        # Apply causal mask if needed\n",
    "        if self.causal:\n",
    "            # Create a causal mask (lower triangular)\n",
    "            mask = torch.triu(torch.ones(L, L, device=x.device), diagonal=1).bool()\n",
    "            attn_BHLL.masked_fill_(mask, float(\"-inf\"))\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attn_BHLL = F.softmax(attn_BHLL, dim=-1)\n",
    "\n",
    "        # Apply dropout to attention weights\n",
    "        attn_BHLL = self.dropout(attn_BHLL)\n",
    "\n",
    "        # 3. Apply attention weights to values\n",
    "        # (B x H x L x L) x (B x H x L x K) -> (B x H x L x K)\n",
    "        # TODO: implement\n",
    "        out_BHLK = None\n",
    "        # END TODO\n",
    "\n",
    "        # 4. Reshape back to B x L x D\n",
    "        out_BLHK = out_BHLK.transpose(1, 2)  # (B x H x L x K) -> (B x L x H x K)\n",
    "        out_BLD = out_BLHK.contiguous().view(B, L, D)  # (B x L x H x K) -> (B x L x D)\n",
    "\n",
    "        # 5. Apply final projection and dropout\n",
    "        # TODO: implement\n",
    "        out_BLD = None\n",
    "        # END TODO\n",
    "        out_BLD = self.dropout(out_BLD)  # (B x L x D) -> (B x L x D)\n",
    "\n",
    "        ### END TODO\n",
    "\n",
    "        return out_BLD, attn_BHLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation by running the following cell\n",
    "# If successful, you should see:\n",
    "# - ✅ All checks passed for Regular Self-Attention!\n",
    "# - ✅ All checks passed for Causal Self-Attention!\n",
    "from week8_tests import test_self_attention_outputs\n",
    "test_self_attention_outputs(SelfAttention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A single Transformer block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can combine the layer norm, self-attention, and feed-forward layers to form a single Transformer block. \n",
    "\n",
    "Note: The particular architecture we are implementing here is also known as the pre-norm Transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, hidden_size, num_heads, intermediate_size, dropout=0.1, causal=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(hidden_size)\n",
    "        self.self_attention = SelfAttention(hidden_size, num_heads, dropout, causal)\n",
    "        self.ln2 = LayerNorm(hidden_size)\n",
    "        self.mlp = MLP(hidden_size, intermediate_size, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape B x L x D\n",
    "        # TODO: implement\n",
    "        attn_BHLL = None\n",
    "        ### END TODO\n",
    "        return x, attn_BHLL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put everything together to form the full Transformer decoder. We start from the embedding layer, then pass through the Transformer blocks, and finally the output layer (LM head)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerForCausalLM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        max_seq_len,\n",
    "        hidden_size,\n",
    "        num_heads,\n",
    "        intermediate_size,\n",
    "        num_layers,\n",
    "        dropout=0.1,\n",
    "        ignore_index=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ignore_index = ignore_index\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(vocab_size, hidden_size),\n",
    "                wpe=nn.Embedding(max_seq_len, hidden_size),\n",
    "                dropout=nn.Dropout(dropout),\n",
    "                blocks=nn.ModuleList(\n",
    "                    [\n",
    "                        TransformerBlock(\n",
    "                            hidden_size,\n",
    "                            num_heads,\n",
    "                            intermediate_size,\n",
    "                            dropout,\n",
    "                            causal=True,  # for a causal model this needs to be set to True\n",
    "                        )\n",
    "                        for _ in range(num_layers)\n",
    "                    ]\n",
    "                ),\n",
    "                ln_f=LayerNorm(hidden_size),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Create the LM head\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "        # Weight tying\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        # Initialize the weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        ### END TODO\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        # Initialize the weights of the model\n",
    "        # Depending on the type of module, we might want to initialize the weights differently\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Initialize linear layers\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # Initialize embeddings\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            # Initialize layer norm (though often this is already handled in the class)\n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        # x is of shape B x L\n",
    "        B, L = x.shape\n",
    "        # Create position indices tensor with proper shape\n",
    "        pos_BL = (\n",
    "            torch.arange(0, L, dtype=torch.long, device=x.device)\n",
    "            .unsqueeze(0)\n",
    "            .expand(B, L)\n",
    "        )\n",
    "\n",
    "        # 1. Construct the embeddings\n",
    "        # TODO: implement\n",
    "        h_BLD = None\n",
    "        # END TODO\n",
    "        h_BLD = self.transformer.dropout(h_BLD)\n",
    "\n",
    "        # 2. Pass through the Transformer blocks\n",
    "        hidden_states = []  # we collect the hidden states for each block\n",
    "        attention_weights = []  # we collet the attention weights for each block\n",
    "        for block in self.transformer.blocks:\n",
    "            # TODO: implement\n",
    "            h_BLD, attn_BHLL = None, None\n",
    "            # END TODO\n",
    "            hidden_states.append(h_BLD)\n",
    "            attention_weights.append(attn_BHLL)\n",
    "\n",
    "        # 3. Apply the final layer norm\n",
    "        h_BLD = self.transformer.ln_f(h_BLD)\n",
    "\n",
    "        # 4. Compute the logits (and optionally the loss)\n",
    "        # TODO: implement\n",
    "        logits_BLV = None\n",
    "        # END TODO\n",
    "\n",
    "        # Compute the loss if targets are provided\n",
    "        loss_BL = None\n",
    "        if targets is not None:\n",
    "            loss_BL = F.cross_entropy(\n",
    "                logits_BLV.view(-1, logits_BLV.size(-1)),\n",
    "                targets.reshape(-1),\n",
    "                ignore_index=self.ignore_index,\n",
    "            )\n",
    "\n",
    "        return h_BLD, logits_BLV, loss_BL, hidden_states, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Transformer-based language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train a Transformer-based language model on the Brown corpus from the exercise of week 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import Tokenizer, SentenceDataset, collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the Brown corpus\n",
    "sents = brown.tagged_sents(categories=\"fiction\")\n",
    "tokenizer = Tokenizer(sents, vocab_size=10000)\n",
    "dataset = SentenceDataset(sents, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = dataset[1]\n",
    "tokens = tokenizer.indices_to_tokens(token_ids)\n",
    "print(token_ids)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a config for the model\n",
    "configs = {\n",
    "    \"tiny\": {\n",
    "        \"hidden_size\": 128,\n",
    "        \"num_heads\": 4,\n",
    "        \"intermediate_size\": 128,\n",
    "        \"num_layers\": 4,\n",
    "        \"dropout\": 0.1,\n",
    "        \"max_seq_len\": 20,\n",
    "    },\n",
    "    \"small\": {\n",
    "        \"hidden_size\": 128,\n",
    "        \"num_heads\": 8,\n",
    "        \"intermediate_size\": 256,\n",
    "        \"num_layers\": 8,\n",
    "        \"dropout\": 0.1,\n",
    "        \"max_seq_len\": 20,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    optimizer,\n",
    "    dataset,\n",
    "    tokenizer,\n",
    "    num_epochs=2,\n",
    "    shuffle_every_epoch=True,\n",
    "    batch_size=8,\n",
    "):\n",
    "    losses_per_epoch = {}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        losses_per_epoch[f\"epoch-{epoch}\"] = []\n",
    "\n",
    "        # Create a dataloader\n",
    "        # We re-create the dataloader for each epoch to ensure that the data is shuffled differently each time\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=partial(\n",
    "                collate_fn, tokenizer=tokenizer, max_len=20, pad=\"right\"\n",
    "            ),\n",
    "            shuffle=shuffle_every_epoch,\n",
    "        )\n",
    "\n",
    "        # Perform a single epoch of training\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch + 1} -- Steps\"):\n",
    "            # Create inputs and targets\n",
    "            inputs = batch[:, :-1]\n",
    "            targets = batch[:, 1:]  # targets are the next tokens in the sequence\n",
    "\n",
    "            # Run forward pass\n",
    "            optimizer.zero_grad()\n",
    "            hidden_state, logits, loss, hidden_states, attention_weights = (\n",
    "                model.forward(inputs, targets)\n",
    "            )\n",
    "            losses_per_epoch[f\"epoch-{epoch}\"].append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        del dataloader\n",
    "\n",
    "    return losses_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 8\n",
    "shuffle_every_epoch = True\n",
    "\n",
    "losses_per_model = {}\n",
    "trained_models = {}\n",
    "\n",
    "for model_type in configs.keys():\n",
    "    # Initialize the model\n",
    "    config = configs[model_type]\n",
    "    model = TransformerForCausalLM(\n",
    "        vocab_size=len(tokenizer.vocabulary),\n",
    "        max_seq_len=config[\"max_seq_len\"],\n",
    "        hidden_size=config[\"hidden_size\"],\n",
    "        num_heads=config[\"num_heads\"],\n",
    "        intermediate_size=config[\"intermediate_size\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        dropout=config[\"dropout\"],\n",
    "    )\n",
    "\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    num_params = num_params / 1e6\n",
    "    print(f\"Training model {model_type} with {num_params:.2f}M parameters\")\n",
    "\n",
    "    # Create an optimizer\n",
    "    learning_rate = 0.001\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    model.train()  # set the model to training mode\n",
    "    losses_per_epoch = train_model(\n",
    "        model,\n",
    "        optimizer,\n",
    "        dataset,\n",
    "        tokenizer,\n",
    "        num_epochs,\n",
    "        shuffle_every_epoch,\n",
    "        batch_size,\n",
    "    )\n",
    "    losses_per_model[model_type] = losses_per_epoch\n",
    "    trained_models[model_type] = model\n",
    "\n",
    "    del optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the losses for each model\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "for model_type, losses_per_epoch in losses_per_model.items():\n",
    "    values_per_epoch = list(losses_per_epoch.values())\n",
    "    values = np.array(values_per_epoch).flatten()\n",
    "\n",
    "    ax.plot(values, label=model_type)\n",
    "\n",
    "    # indcate when a new epoch starts\n",
    "    for i, losses in enumerate(values_per_epoch):\n",
    "        ax.axvline(x=len(losses) * i, color=\"k\", linestyle=\"--\")\n",
    "\n",
    "ax.set_xlabel(\"Steps\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Training loss\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference and check predictions\n",
    "\n",
    "text = \"Scotty did not go back to\" # Let's take a sentence from the Brown corpus\n",
    "\n",
    "# Tokenize the text\n",
    "token_ids = tokenizer.encode(text)\n",
    "tokens = tokenizer.indices_to_tokens(token_ids)\n",
    "# print(token_ids)\n",
    "# print(tokens)\n",
    "\n",
    "# Convert to tensor\n",
    "token_ids = torch.tensor(token_ids).unsqueeze(0) # make it a batch of size 1\n",
    "# print(token_ids)\n",
    "\n",
    "# Run inference\n",
    "logits_per_model = {}\n",
    "for model_type, model in trained_models.items():\n",
    "    print(f\"Running inference with {model_type} model\")\n",
    "    model.eval() # set the model to evaluation mode\n",
    "    hidden_state, logits, loss, hidden_states, attention_weights = model.forward(token_ids)\n",
    "    logits_per_model[model_type] = logits.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert logits to probabilities\n",
    "probs_per_model = {k: F.softmax(v, dim=-1) for k, v in logits_per_model.items()}\n",
    "\n",
    "# Get the predicted token\n",
    "predicted_token_per_model = {k: torch.argmax(v, dim=-1) for k, v in probs_per_model.items()}\n",
    "\n",
    "for model_type, predictions in predicted_token_per_model.items():\n",
    "    print(f\"Model: {model_type}\")\n",
    "    for batch_idx, prediction in enumerate(predictions):\n",
    "        predicted_tokens = tokenizer.indices_to_tokens(prediction.tolist())\n",
    "        print(f\"Predicted tokens: {predicted_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the distribution over the vocabulary at every position in the sentence\n",
    "for model_type, predictions in predicted_token_per_model.items():\n",
    "    print(f\"Model: {model_type}\")\n",
    "    for batch_idx, probs in enumerate(probs_per_model[model_type]):\n",
    "        for pos_idx, prob in enumerate(probs):\n",
    "            print(prob.shape)\n",
    "            # make sure the probabilities sum to 1\n",
    "            assert torch.allclose(prob.sum(), torch.tensor(1.0)), prob.sum()\n",
    "            \n",
    "            # visualize the probability distribution\n",
    "            fig, ax = plt.subplots(figsize=(6, 4))\n",
    "            ax.plot(range(len(prob)), prob.tolist())\n",
    "            ax.set_title(f\"Model: {model_type}, Batch: {batch_idx}, Position: {pos_idx}\")\n",
    "            ax.set_yscale(\"log\")\n",
    "\n",
    "            # annotate witht the argmax \n",
    "            argmax = torch.argmax(prob).item()\n",
    "            argmax_token = tokenizer.idx_to_token(argmax)\n",
    "            ax.annotate(argmax_token, (argmax, prob[argmax]), textcoords=\"offset points\", xytext=(50, -10), ha='center')\n",
    "\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface 🤗 transformers is a library that provides a wide range of pre-trained models for natural language processing (NLP). It includes models for language modeling, text classification, question answering, and more.It is the defacto standard for working with NLP models in PyTorch and is widely used in academia and industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, GPTNeoXForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is a library that provides a wide range of datasets in a unified format. It is useful for loading and preprocessing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-raw-v1\", split=\"test\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-14m\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model comes with its own tokenizer. The tokenizer class handles all data preprocessing, including tokenization, detokenization, and encoding/decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-14m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Scotty did not go back to\"\n",
    "token_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids) \n",
    "print(token_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation = model.generate(torch.tensor(token_ids).unsqueeze(0), max_length=20)\n",
    "print(generation)\n",
    "tokenizer.decode(generation[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

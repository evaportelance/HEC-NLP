{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf3830e5-32fd-464d-b4d0-c8fb733ebea0",
   "metadata": {},
   "source": [
    "# Week 2 Exercises: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0860ece9-f2bd-41f2-b691-0018bb552c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import Counter\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f3f158-e991-4c44-bca7-028bbc9dec7b",
   "metadata": {},
   "source": [
    "## RegEx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2939f18-6c4a-4750-a63a-957337038735",
   "metadata": {},
   "source": [
    "In python we use the 're' library to work with regular expressions. A good place to test out regular expressions and what they capture is [https://regexr.com/](https://regexr.com/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabbaa9e-d905-4345-8643-3ce5c66ba814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbfbe0b-937f-4d3d-9bbd-022da285ab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = brown.words('cj01')\n",
    "text = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e1c7d-02cf-45b8-8bf9-a107e7b3d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403e88ba-af6a-4948-b78a-248a79720501",
   "metadata": {},
   "source": [
    "1. Find all words starting with a capital letter in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9fa01e-e5db-46d0-9e0a-ccd46f9dc6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cap_words(text):\n",
    "    cap_words = []\n",
    "    ## TO DO\n",
    "    \n",
    "    ##\n",
    "    return cap_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57c1bd5-6e2f-46b7-80c9-6e6b24ce4634",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_cap_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad73adea-eee5-4b9c-87ed-afbe3b212b2f",
   "metadata": {},
   "source": [
    "2. Find all the digits with decimals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c029fe1-80f6-421d-a990-d50e400cd757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_decimal_numbers(text):\n",
    "    numbers = []\n",
    "    ## TO DO\n",
    "    \n",
    "    ##\n",
    "    return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b08942-f966-4d33-a2a5-948f568d11f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_decimal_numbers(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba2cb89-1e56-466b-aec1-6a298ccf6fb9",
   "metadata": {},
   "source": [
    "## BPE Tokenization\n",
    "\n",
    "Let's implement BPE tokenization and look at how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34220581-8d10-4e80-b5a6-632db8e55a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = brown.words('cj01')\n",
    "text = text = ' '.join(words[:216])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff238929-90dd-4d01-b5a4-e774b75ebb0d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. The following functions are helper functions you will need to complete for the BPE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bfd428-1ea2-4335-b9c4-401a33b4d900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_characters(text):\n",
    "    vocabulary = []\n",
    "    tokenized_text = []\n",
    "    ## TO DO\n",
    "\n",
    "    ##\n",
    "    return vocabulary, tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a70fbb-37c8-4092-93d4-de7b59dc3553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_frequent_pair(tokenized_text):\n",
    "    ## TO DO\n",
    "\n",
    "    ##\n",
    "    return (left_type,right_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2935c582-244b-411d-8e6e-7371a28b4e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_new_type(tokenized_text, pair, new_type):\n",
    "    new_tokenized_text = []\n",
    "    ## TO DO\n",
    "\n",
    "    ## \n",
    "    return new_tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a892c5-cea7-47ff-a40c-b5281bf3f4a0",
   "metadata": {},
   "source": [
    "2. Here is the BPE algorithm, lets look at what happens at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0fe5a1-396f-4720-9190-9bb655410bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BPE_tokenizer(text, vocab_size):\n",
    "    # 1. initialize vocabulary as set of all unique characters in text and tokenized text via character tokenization\n",
    "    vocabulary, tokenized_text = get_all_characters(text)\n",
    "    #To better understand how BPE works, we'll record the its states and then look at them after\n",
    "    states = [(copy.deepcopy(vocabulary), copy.deepcopy(tokenized_text))]\n",
    "    # 2. Repeat merge steps until termination condition\n",
    "    while len(vocabulary) < vocab_size:\n",
    "        # 2.1 find most frequent pair of adjacent types\n",
    "        pair = find_most_frequent_pair(tokenized_text)\n",
    "        # 2.2 update vocabulary with new type\n",
    "        new_type = ''.join(pair)\n",
    "        vocabulary.append(new_type)\n",
    "        # 2.3 replace all all occurences of pair with new type in corpus\n",
    "        tokenized_text = replace_new_type(tokenized_text, pair, new_type)\n",
    "        states.append((copy.deepcopy(vocabulary), copy.deepcopy(tokenized_text)))\n",
    "    return vocabulary, tokenized_text, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc2b8c3-d847-41dd-9c1a-ece8484642be",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, tokenized_text, states = BPE_tokenizer(text, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d053d364-a3a2-4693-a089-915588334aee",
   "metadata": {},
   "source": [
    "Now that we've run our tokenizer, here is the final 60 token vocabulary and the tokenized paragraph. Do you notice anything interesting about them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15940fa-f401-45b1-b893-9aa1d8ec875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vocabulary: ' +str(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42322f8-4608-4aed-a264-cac5daecaa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tokenized text: ' +str(tokenized_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c106e5-152c-46e0-8d8e-087cc1492414",
   "metadata": {},
   "source": [
    "To better understand how these were derived, lets look at what happens to the vocabulary at each iterative merge call in the BPE algorithm.\n",
    "\n",
    "First, here is the initial state with all of the individual characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb52936c-2e08-4710-be0c-5541bc9a2bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(states[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c7d7e4-8ebe-48f9-8efc-ec031d8054b0",
   "metadata": {},
   "source": [
    "Now here are the subsequent five states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c7aa46-8036-4195-b93d-e388b978b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(states[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fccd79-1877-416e-89c6-013da8833255",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(states[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2594f0-fd48-42d1-b442-2adad74f018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(states[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d61c6c-c802-4c8c-ba7d-7cf83cdab071",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(states[4][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6421097b-47b1-403a-8962-d03a218fddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(states[5][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

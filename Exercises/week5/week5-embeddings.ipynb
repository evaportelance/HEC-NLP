{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b911869-3428-4db3-84d5-40c7ea7450de",
   "metadata": {},
   "source": [
    "# Week 5: Skip-gram, dense embeddings, and semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea366a7-9497-44cb-b0c5-7d729798ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d36a0-1392-4019-a650-8ed774bfe696",
   "metadata": {},
   "source": [
    "First let's get our tokenized dataset and vocabulary for which we will learn embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57c1b11-4ff3-4ccf-b828-35fd95b1b908",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = [token.lower() for token in brown.words() if re.match(r'\\w', token)]\n",
    "len(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43bcb38-0b96-461a-a7c4-03fd29c9ea9b",
   "metadata": {},
   "source": [
    "We will limit ourselved to tokens which appear at least 8 times in the dataset, for a total of 10 000 embeddings or so, all other tokens will be replaced by an unknown token, or '\\<UNK>' which we have made sure to add to our vocabulary at index 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da59c60-21eb-44b0-9767-c0924a68dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = dict(Counter(tokenized_dataset))\n",
    "vocab = [token for token, count in counts.items() if count >= 8]\n",
    "vocab  = ['<UNK>'] + vocab\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22447c77-9819-4682-a81f-259b89014752",
   "metadata": {},
   "source": [
    "## Datasets and DataLoaders\n",
    "\n",
    "Now let's introduce a dataset class which will be used to store our data, create word to index mappings and generate positive and negative samples for our Skipgram training algorithm. This new class inherits from torch.utils.data.Dataset, a useful class type which can be combined with dataloader objects to make batched data generation easier with pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bada9329-e5d4-43d7-8309-6bb1973e2d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramDataset(Dataset):\n",
    "    def __init__(self, tokenized_dataset, vocab, context_size, k_negative_sample_size):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "        self.vocab = vocab\n",
    "        self.context_size = context_size\n",
    "        self.k_negative_sample_size = k_negative_sample_size\n",
    "        self.words_to_indices = {word:i for i,word in enumerate(vocab)}\n",
    "        self.indexed_dataset = [self.words_to_indices[token] if token in self.words_to_indices else self.words_to_indices['<UNK>'] for token in self.tokenized_dataset]\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.skipgram_items = []\n",
    "        self.get_skipgram_items()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pos_samples = self.skipgram_items[idx]\n",
    "        neg_samples = self.get_negative_samples(pos_samples)\n",
    "        return pos_samples, neg_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.skipgram_items)\n",
    "\n",
    "    def get_skipgram_items(self):\n",
    "        for i in range(len(self.indexed_dataset)-self.context_size):\n",
    "            window = self.indexed_dataset[i:(i+self.context_size*2+1)]\n",
    "            target = window[self.context_size]\n",
    "            if target != 0:\n",
    "                context = window[:self.context_size] + window[self.context_size+1:]\n",
    "                for c_token in context:\n",
    "                    self.skipgram_items.append((target, c_token))\n",
    "\n",
    "    def get_negative_samples(self, pos_samples):\n",
    "        target, context = pos_samples\n",
    "        neg_samples = []\n",
    "        while len(neg_samples) < self.k_negative_sample_size:\n",
    "            neg_context = random.sample(range(0, self.vocab_size), 1)\n",
    "            if neg_context != target:\n",
    "                neg_samples.append((target,neg_context[0]))\n",
    "        return neg_samples\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba348049-3350-4663-85c0-26341dd3bcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset and loader hyperparameters\n",
    "window_size = 3\n",
    "k_negative_samples = 5\n",
    "batch_size = 200\n",
    "\n",
    "data = SkipgramDataset(tokenized_dataset, vocab, window_size, k_negative_samples)\n",
    "dataloader = DataLoader(data, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7584a01d-4735-4e03-bff6-d18491507c59",
   "metadata": {},
   "source": [
    "## Models and Modules\n",
    "\n",
    "Here is a quick helper function to flatten of lists of k negative samples into a uniform batch that can be processed by our model the same way as positive samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d497ce-4a80-48f4-87a1-da2e045f0794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_neg_samples(neg_samples):\n",
    "    neg_samples_flat = [torch.IntTensor([]), torch.IntTensor([])]\n",
    "    for target, context in neg_samples:\n",
    "        neg_samples_flat[0] = torch.cat((neg_samples_flat[0], target))\n",
    "        neg_samples_flat[1] = torch.cat((neg_samples_flat[1], context))\n",
    "    return neg_samples_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623512f0-e5e9-4757-8911-d6aeefa10d42",
   "metadata": {},
   "source": [
    "Here is our SkipGram model which inherits from torch.nn.Module. You will notice that it simply contains the target and context embeddings and applies a dot product between them which it then sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64c0dd4-acf2-407d-96aa-cb1765c7e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram_Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGram_Model, self).__init__()\n",
    "        self.target_embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "        self.context_embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        emb_target = self.target_embeddings(inputs[0])\n",
    "        emb_context = self.context_embeddings(inputs[1])\n",
    "        output = torch.sum(emb_target * emb_context, dim=1)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0e014c-daa3-47d1-a737-4d77e52e4a1d",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "Next we have to define our learning objective, or loss function. In this implementation of skipgram we are using negative sampling to approximate cross entropy loss. We follow the formula given in the class slides and finally sum our results for both positive and negative samples across each batch. A perfect cross entropy loss is 0, so we want to minimize this return value and try to get it closer and closer to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec74c8eb-a3ec-4afe-8dce-31bc90138da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram_NegativeSamplingLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SkipGram_NegativeSamplingLoss, self).__init__()\n",
    "        self.log_sigmoid = nn.LogSigmoid()\n",
    "\n",
    "    def forward(self, pos_logits, neg_logits):\n",
    "        pos_loss = torch.sum(self.log_sigmoid(pos_logits), dim=0)\n",
    "        neg_loss = torch.sum(self.log_sigmoid(- neg_logits), dim=0)\n",
    "        return - (pos_loss + neg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa80a427-a921-4387-8b8f-02dc492e9b4c",
   "metadata": {},
   "source": [
    "## Training Procedures\n",
    "\n",
    "Now for the training procedure. Training this skipgram model takes time, so I have run it in advance and cashed the results for you. Nevertheless, let's walk through the steps involved in this training procedure. First we define the hyperparameters needed for the model and training loop: the learning rate and number of epochs, as well as our embedding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d56f1-be5a-4d75-8692-d7431cb7ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and model hyperparameters\n",
    "lr = 0.001\n",
    "epochs = 2\n",
    "emb_size = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363a110c-a93a-40a7-8efc-69fa5ba0f7b5",
   "metadata": {},
   "source": [
    "Next we need to initialize our model, optimizer and loss function, often called \"criterion\" in scripts. You already know what the model and loss function do, but what about the optimizer? The optimizer will be in charge of updating the model weights during training. This is why it takes the model parameters as its input. There are different types of optimization function, here we use Adam, an form of stochastic gradient descent with a learning rate of 0.001, the constant that we multiply our gradient updates by to slow learning to avoid over fitting to specific batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3fb41c-c297-47e5-832a-5c417b72e43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkipGram_Model(data.vocab_size, emb_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = SkipGram_NegativeSamplingLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ada58a2-01c2-4857-8804-7cc8afd97f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scratch = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2895b6da-c422-4bf9-9b2b-2bf6549a7dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_scratch:\n",
    "    losses =[]\n",
    "    mean_loss = 0\n",
    "    i=0\n",
    "    \n",
    "    # set the model to train mode\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for pos_samples, neg_samples in dataloader:\n",
    "            # reformat negative samples into a single k*batch_size batch\n",
    "            neg_samples = flatten_neg_samples(neg_samples)\n",
    "            # clear gradients from optimizer\n",
    "            optimizer.zero_grad()\n",
    "            # get logits for both negative and positive samples\n",
    "            pos_logits = model(pos_samples)\n",
    "            neg_logits = model(neg_samples)\n",
    "            # calculate the loss\n",
    "            loss = criterion(pos_logits, neg_logits)\n",
    "            # keep track of mean loss so we can plot it after\n",
    "            mean_loss+=loss.item()\n",
    "            if i == 0 or i%100 == 0 :\n",
    "                losses.append((i,float(mean_loss/100)))\n",
    "                mean_loss = 0\n",
    "            i+=1\n",
    "            # calculate the gradients\n",
    "            loss.backward()\n",
    "            # add the gradients to model parameters based on learning rate\n",
    "            optimizer.step()\n",
    "\n",
    "    sum_embeddings = model.target_embeddings.weight.detach().numpy() + model.context_embeddings.weight.detach().numpy()\n",
    "    target_embeddings = model.target_embeddings.weight.detach().numpy()\n",
    "\n",
    "    np.save('target_embeddings.npy', target_embeddings) \n",
    "    np.save('sum_embeddings.npy', sum_embeddings) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fdb2e4-f816-4a1a-804d-5aeec181e602",
   "metadata": {},
   "source": [
    "## Similarity and embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aca232c-013f-4467-bbe3-2722e3848406",
   "metadata": {},
   "source": [
    "Now that we have trained our model, we can extract the embeddings from it. We have two options: we can either simply take the target embeddings, or we can sum the target and the context embeddings together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a14777b-2b3c-4d58-a310-a2935e5dc3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_embeddings = np.load('target_embeddings.npy')\n",
    "sum_embeddings = np.load('sum_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bc987c-1c50-4e7f-a27d-0201c9c89cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0644ecf1-575a-447f-86a6-6f003dee5b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7358fe5e-65bd-45f8-9bfe-a9d427a6f948",
   "metadata": {},
   "source": [
    "Here is a helper function to find similar the top n words to a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da0e1a2-bc41-4edb-a67f-9850c2402d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get similar words\n",
    "def get_similar_words(word, words_to_indices, vocab, embeddings, top_n=10):\n",
    "    idx = words_to_indices[word]\n",
    "    word_embedding = embeddings[idx]\n",
    "    similarities = np.dot(embeddings, word_embedding)\n",
    "    closest_idxs = (-similarities).argsort()[1:top_n+1]\n",
    "    return [vocab[idx] for idx in closest_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3eac6a-63b6-4fd8-8831-fbb60e06587a",
   "metadata": {},
   "source": [
    "Try playing around with it by searching for different words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abb18d5-2075-412b-a66a-84c7dff160ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar_words(\"queen\", data.words_to_indices, data.vocab, target_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6ea930-be5b-42bd-9489-96032a41be3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar_words(\"queen\", data.words_to_indices, data.vocab, sum_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a29701-0732-4bf3-9292-fbb7d19658b5",
   "metadata": {},
   "source": [
    "NLTK also provides a set of pretrained Word2Vec embeddings trained on the google book corpus. Lets load them in and play around with them as well. [https://www.nltk.org/howto/gensim.html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9557695-d911-460b-9a18-f9ddb8c9bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.data import find\n",
    "from nltk.test.gensim_fixt import setup_module\n",
    "setup_module()\n",
    "import gensim\n",
    "nltk.download('word2vec_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7fc0e4-2008-400c-acb9-a9c819d45425",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "pretrained_embeddings = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ab7e0a-a65b-4451-b9bb-cc04c112694d",
   "metadata": {},
   "source": [
    "The following will do the same as our get_similar_words helper function but for these pretrained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f278a649-e718-4833-84d0-f2ba86c8b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings.most_similar(positive=['king'], topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40791a2f-8cf1-495a-8f5d-2003a16d0c0e",
   "metadata": {},
   "source": [
    "This usage will apply the parallelogram rule we saw in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf03857-1bec-49ad-871d-ccbcb1147721",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings.most_similar(positive=['woman','king'], negative=['man'], topn = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3eec76-22e3-4340-a56e-26a6980adffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

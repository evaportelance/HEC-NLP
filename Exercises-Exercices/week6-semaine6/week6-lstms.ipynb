{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c35bf1c5-f366-4ef9-b2b9-bb5cd5eaa612",
   "metadata": {},
   "source": [
    "# Week 6: RNNs and LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7842421f-1c3b-42b3-b689-c9bfd49f7328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f8bd55-c717-49d7-8828-bfe724c3d3bd",
   "metadata": {},
   "source": [
    "## LSTM for sequence labelling\n",
    "\n",
    "We are going to train an LSTM-based POS tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b0a82-8e68-4262-a1a0-b668dda5d38c",
   "metadata": {},
   "source": [
    "### Model \n",
    "\n",
    "First let's define the model. I have already included the relevant layers. You must now fill in the missing part of the forward function which defines the relation between each layer. Refer to the class slides on LSTMs and remember that we are using this RNN for sequence labelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae7318-cf87-48b1-825f-981fe3665c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMSeqLabeller(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, output_dim):\n",
    "        super(LSTMSeqLabeller, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.emb_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTMCell(self.emb_dim, self.hidden_dim)\n",
    "        self.linear = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input is of size (batch, time_steps)\n",
    "        batch_size = input.size()[0]\n",
    "        time_steps = input.size()[1]\n",
    "        #our initial h_(i-1) and c_(i-1) vectors are dummies\n",
    "        h_prior = torch.zeros(batch_size, self.hidden_dim)\n",
    "        c_prior = torch.zeros(batch_size, self.hidden_dim)\n",
    "        #outputs should become a list of |time_steps| tensors of size (batch, output_dim)\n",
    "        outputs = []\n",
    "        input_t = torch.transpose(input, 0, 1)\n",
    "        for i in range(time_steps):\n",
    "            ## TO DO\n",
    "\n",
    "            \n",
    "            ##\n",
    "        # we must now turn our list of tensors for each state back into a single tensor of size (batch, output_size, time_step)\n",
    "        # This is because our loss criterion will expect model logits of (Batch_size x num_classes x any other dimensions...)\n",
    "        output = torch.reshape(torch.stack(outputs, dim=0), (batch_size, self.output_dim, time_steps))\n",
    "        return output "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948610ed-b4ee-4160-b720-9ddaca053495",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Next we need to prepare our data for our Dataset and DataLoader. Here is the brown corpus split into sentences where each word is tagged with a POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5093cf6e-980b-4a15-ade7-638c6564fa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sents = brown.tagged_sents(categories='fiction')\n",
    "#tagged_sents = brown.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e12deea-73ce-410d-90b0-2903a4bf065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8d1294-4c9c-452f-9cf4-df4b1e909215",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db4673-3fa5-45ed-b015-6e7aa866c5db",
   "metadata": {},
   "source": [
    "We are going to limit our vocabulary size to the 10,000 most frequent words and replace all other words by '\\<UNK>'. We will also have a special '\\<PAD>' token. '\\<PAD>' will be our index 0 vocabulary item, and '\\<UNK>' our index 1. All real tokens will then follow. So our final vocabulary of unique tokens, or types, should have length vocab_size + 2, here 10002. Let's now write a function that will return the unique vocabulary of tokens in a dataset of tagged sentences, as well as its corresponding list of unique possible POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b633426-e0ac-43b6-94b4-b35f4a887013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_taglist(tagged_sents, vocab_size = 2000, unk = '<UNK>', pad = '<PAD>'):\n",
    "    vocabulary = [pad, unk]\n",
    "    tag_list = [pad, unk]\n",
    "    ## TO DO\n",
    "\n",
    "    \n",
    "    ##\n",
    "    return vocabulary, tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b52c4d-c3ad-4f55-92af-7e971f1f8f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, tag_list = create_vocab_taglist(tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc351e-6cfe-4b73-bed7-4ba70940683e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf16a3-492b-40c7-af7b-6dc5ce9749df",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tag_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43359822-fc36-4e22-84d1-ce01778f9d5a",
   "metadata": {},
   "source": [
    "Now we can define our custom dataset class. You will need to fill in the get_indexed_tagged_sents method that will populate the datasets indexed_tagged_sents property. This should be a list of sentences, where each sentence is a list of (token_idx, tag_idx) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21a1a4f-26e7-4a1d-94d8-a06fb6d64126",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TagSentDataset(Dataset):\n",
    "    def __init__(self, tagged_sents, vocabulary, tag_list):\n",
    "        self.tagged_sents = tagged_sents\n",
    "        self.vocabulary = vocabulary\n",
    "        self.tag_list = tag_list\n",
    "        self.tokens_to_idx = {k:v for v, k in enumerate(vocabulary)}\n",
    "        self.idx_to_tokens = {k:v for k, v in enumerate(vocabulary)}\n",
    "        self.indexed_tagged_sents = []\n",
    "        self.get_indexed_tagged_sents()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        sent = self.indexed_tagged_sents[idx]\n",
    "        inputs, labels = list(zip(*sent))\n",
    "        return torch.tensor(inputs, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indexed_tagged_sents)\n",
    "\n",
    "    def get_indexed_tagged_sents(self):\n",
    "        ## TO DO\n",
    "\n",
    "        \n",
    "        ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaff68b-f36c-4479-9af7-ad18bb924db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TagSentDataset(tagged_sents, vocabulary, tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff60fd8-92cb-4c40-8e56-ab08d2d7dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24fe7b2-29f2-4be4-9897-349ca97ad8f4",
   "metadata": {},
   "source": [
    "Before we initialize our dataloader, we will need to write a custom collate function. The collate function can be passed to the dataloader as a parameter. This function serves the purpose of standardizing all the items in a batch. Because each of our items is a sentence and that sentences can be of different length, we need to standardize the length of all sentences in a batch so that they may then be stacked into a single tensor. We will do this by adding '\\<PAD>' tokens to the beginning of sentences up to the length of the longest sentence in the batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8588ad6-fea3-4320-964f-c58a3c7a0422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(items):\n",
    "    inputs, labels = list(zip(*items))\n",
    "    max_len = max([len(sent) for sent in inputs])\n",
    "    batched_inputs = []\n",
    "    batched_labels = []\n",
    "    for sent, tags in zip(inputs, labels):\n",
    "        n_pads = max_len - len(sent)\n",
    "        pads = torch.zeros(n_pads, dtype=torch.long)\n",
    "        pad_sent = torch.cat((pads, sent))\n",
    "        pad_tags = torch.cat((pads, tags))\n",
    "        batched_inputs.append(pad_sent)\n",
    "        batched_labels.append(pad_tags)\n",
    "    batched_inputs = torch.stack(batched_inputs)\n",
    "    batched_labels = torch.stack(batched_labels)\n",
    "\n",
    "    return batched_inputs, batched_labels     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e28217-094b-41ef-933b-bb29db929b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(data, batch_size = 32, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ff7163-3f78-4ffb-bdd4-3fc820d0fde8",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f1749b-b8e0-4cb1-9b21-c2d450dc0e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "vocab_size = len(data.vocabulary)\n",
    "emb_dim = 100\n",
    "hidden_dim = 50\n",
    "output_dim = len(data.tag_list)\n",
    "\n",
    "# Training Hyperparameters\n",
    "epochs = 10\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed444d50-0fc7-4e7a-bf6e-b183ac7f5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMSeqLabeller(vocab_size, emb_dim, hidden_dim, output_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607130d8-11d2-4a68-9a21-71678fd71cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data = []\n",
    "step = 1\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in dataloader:\n",
    "        # clear gradients from optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # get logits from model\n",
    "        logits = model(inputs)\n",
    "        # calculate the loss\n",
    "        loss = criterion(logits, labels)\n",
    "        # keep track of loss so we can plot it after\n",
    "        loss_data.append((step, loss.item()))\n",
    "        step+=1\n",
    "        # calculate the gradients\n",
    "        loss.backward()\n",
    "        # add the gradients to model parameters based on learning rate\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dcfa6c-c814-4b77-b1fa-25ffb6963213",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*zip(*loss_data))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91fa0af-41f9-47e6-a73d-eef07de5f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_sentence(sent, data):\n",
    "    tokens = sent.split(\" \")\n",
    "    indexed_sent = []\n",
    "    for t in tokens :\n",
    "        idx = data.tokens_to_idx[t] if t in data.tokens_to_idx else 1\n",
    "        indexed_sent.append(idx)\n",
    "    return torch.tensor([indexed_sent], dtype=torch.long)    \n",
    "\n",
    "def model_predict(x, model, data):\n",
    "    model.eval()\n",
    "    logits = model(x)\n",
    "    y_hat = F.softmax(logits, dim=1)\n",
    "    _, preds = torch.max(y_hat, dim=1)\n",
    "    return preds\n",
    "\n",
    "def retrieve_sequence(pred, data):\n",
    "    sequence = []\n",
    "    for idx in pred:\n",
    "        sequence.append(data.tag_list[idx])\n",
    "    return sequence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e350cc5-2db4-4449-a9ab-e3cddbf1ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = prep_sentence('There was a child .', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a937c6-34cd-4b27-88ec-d3a1143f9c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model_predict(x, model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e671877-ffb2-4d01-83a1-5b4f8fd224fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_sequence(preds[0], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b4c6a9-bcf2-40b3-baa5-8aacd0a65004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

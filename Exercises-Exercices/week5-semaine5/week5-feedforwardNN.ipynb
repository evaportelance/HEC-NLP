{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923f7fa8-2c19-4d5e-a5c9-f98b3ec427fb",
   "metadata": {},
   "source": [
    "# Week 5: Feedforward neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fca0d0-9a34-4c91-9f54-f036cf1c753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.data import find\n",
    "import gensim\n",
    "from nltk.corpus import brown\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd375a7-83db-4e86-b30d-535508343ad0",
   "metadata": {},
   "source": [
    "We are going to implement the 2-layered feedforward model from today's slides. Here is its architecture:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24852495-2344-42be-894d-66e06920e3bd",
   "metadata": {},
   "source": [
    "<img src=\"NN-2layer.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bdd853-5ead-4904-9210-ab56706822bb",
   "metadata": {},
   "source": [
    "Fill in both the initialization function and the forward function to replicate this model's layers, you should only include the layers in the purple box as the final sigmoid activation function will be included in the loss function defined below. Assume that the model can have *input_dim* number of input features and that the second layer has *hidden_dim* number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d241038-ec04-4bb3-a9d6-b20fc255af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        ## TO DO\n",
    "\n",
    "    \n",
    "        ##\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## TO DO\n",
    "\n",
    "        \n",
    "        ##\n",
    "        return z2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c718d9e-92d6-4629-94a9-6dc45bad1d69",
   "metadata": {},
   "source": [
    "Next, let's prepare our data, we are going to try to predict the POS tag of words in the news articles in the Brown corpus as a function of their word2vec embedding features. The first step is to get our list of (token,tag) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3aa8b0-0a67-46ed-b697-2e9277560e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_words = [(token.lower(), tag) for (token, tag) in brown.tagged_words(categories='news') if re.match(r'\\w', token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b427121-602a-4b37-8de6-dbd4492afd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238a6fcf-5fab-43f0-8831-27c1d2229aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_words[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00a91cd-c2c4-498b-8cfa-f1336ab3bf59",
   "metadata": {},
   "source": [
    "We can now load in our pretrained word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807563c1-1aa8-4464-ae25-5aa4da6a6f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "pretrained_embeddings = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f78c76-fcba-41de-b2cb-9e4ff4b37650",
   "metadata": {},
   "source": [
    "Let's filter our dataset to only include pairs where an embedding for the token exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c228609-7640-44c7-92aa-1201196b82e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_dataset = [(token, tag) for (token, tag) in tagged_words if token in pretrained_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfdf55a-fe79-4706-b43e-d6bbc7a6584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tagged_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b750f167-bb78-41b9-96d5-5f00e66f0453",
   "metadata": {},
   "source": [
    "Let's get the list of unique tags in our dataset which will serve as our output categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808f2b05-afc6-4e04-9d81-df0a0ae7bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_list = list(set([tag for (token, tag) in tagged_dataset]))\n",
    "len(tag_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671246bd-ca6d-4ed6-98df-7adff917a222",
   "metadata": {},
   "source": [
    "Now let's create our pytorch Dataset. You will need to complete the \\__getitem__ function. Given an index, it should find the (token, tag) pair in the dataset at that index and return *inputs*, which should be a torch tensor of the embedding of the token, as well as the *label*, which should be a torch tensor of type long of the tag's index in tag_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9446bf6-edde-497d-980d-a73cdd6482b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TagDataset(Dataset):\n",
    "    def __init__(self, tagged_dataset, pretrained_embeddings, tag_list):\n",
    "        self.tagged_dataset = tagged_dataset\n",
    "        self.pretrained_embeddings = pretrained_embeddings\n",
    "        self.tag_list = tag_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ## TO DO\n",
    "\n",
    "        \n",
    "        ##\n",
    "        return inputs, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tagged_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02446e8-7072-45a0-b7c4-a17f796bd8fc",
   "metadata": {},
   "source": [
    "Let's create our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9089df-b75c-4da3-9b95-d24c3df593ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TagDataset(tagged_dataset, pretrained_embeddings, tag_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb7646-70f8-41f9-aa7f-727ecff5c68d",
   "metadata": {},
   "source": [
    "Let's now define our hyperparameters. Fill in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcf50b5-97ca-48ef-9a7d-2abfcd68c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "input_dim = ## TO DO ##\n",
    "hidden_dim = 250\n",
    "output_dim = ## TO DO ##\n",
    "\n",
    "# Training Hyperparameters\n",
    "epochs = 3\n",
    "batch_size = 32\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2455ca78-553f-4b0d-a193-be778178ad8b",
   "metadata": {},
   "source": [
    "We can now create our model and dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b308f8f-9c89-46f8-83fe-f56d2dbe4d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_dim, hidden_dim, output_dim)\n",
    "dataloader = DataLoader(data, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13d44d6-0f63-420f-a7c8-332f27d3099d",
   "metadata": {},
   "source": [
    "Next we need to initialize our loss function and the optimizer we will use for backpropagation. Check out this page to see what nn.CrossEntropyLoss implements: [https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cfb01f-5291-4dac-90df-40509ca469ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb6b86c-914f-4e8b-8e71-61d84d74e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data = []\n",
    "step = 1\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in dataloader:\n",
    "        # clear gradients from optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # get logits from model\n",
    "        logits = model(inputs)\n",
    "        # calculate the loss\n",
    "        loss = criterion(logits, labels)\n",
    "        # keep track of loss so we can plot it after\n",
    "        loss_data.append((step, loss.item()))\n",
    "        step+=1\n",
    "        # calculate the gradients\n",
    "        loss.backward()\n",
    "        # add the gradients to model parameters based on learning rate\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b49fb3c-a356-440f-8cd8-bf64212f0f37",
   "metadata": {},
   "source": [
    "Finally, let's plot the loss to see what it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bba056-008a-4092-b493-294c61f98b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*zip(*loss_data))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16556847-2d41-4fd4-9968-9ef93873cfb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
